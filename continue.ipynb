{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94281083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Find all CSV files in the current directory and all subdirectories\n",
    "csv_files = glob.glob('**/*.csv', recursive=True)\n",
    "\n",
    "output_txt = 'csv_inspection_results.txt'  # Name of the output file\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found. Please make sure your CSV files are in the working directory or its subdirectories.\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files. Inspecting each one...\\n\")\n",
    "\n",
    "    # Open the output file once\n",
    "    with open(output_txt, 'w') as f:\n",
    "        f.write(f\"CSV Inspection Report - Found {len(csv_files)} CSV files\\n\\n\")\n",
    "\n",
    "        for file_path in csv_files:\n",
    "            print(\"--------------------------------------------------\")\n",
    "            print(f\"File: {file_path}\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "            try:\n",
    "                # Read the CSV into a pandas DataFrame\n",
    "                df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "                # Write results to the output file\n",
    "                f.write(f\"File: {file_path}\\n\")\n",
    "                f.write(f\"Shape: {df.shape}\\n\\n\")\n",
    "                f.write(\"Head:\\n\")\n",
    "                f.write(df.head().to_string())\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "                print(f\"Results for {file_path} written to {output_txt}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                f.write(f\"File: {file_path}\\n\")\n",
    "                f.write(f\"Error: {e}\\n\\n\")\n",
    "                print(f\"Could not read or process file. Error: {e}\\n\")\n",
    "\n",
    "    print(f\"All results saved to {output_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1184da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# The input file is the co-occurrence matrix you showed me.\n",
    "INPUT_FILE = 'skill_co_occurrence_matrix.csv' \n",
    "OUTPUT_FILE = 'centrality_measures.csv'\n",
    "\n",
    "print(f\"Reading skill relationship data from '{INPUT_FILE}'...\")\n",
    "\n",
    "try:\n",
    "    # Load the co-occurrence matrix. The first column contains the skill names, so it becomes the index.\n",
    "    df = pd.read_csv(INPUT_FILE, index_col=0)\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: The input file '{INPUT_FILE}' was not found.\")\n",
    "    print(\"Please make sure this script is in the same directory as your CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# The networkx library works best with clean column names. Let's ensure they are all strings.\n",
    "df.columns = df.columns.astype(str)\n",
    "df.index = df.index.astype(str)\n",
    "\n",
    "print(\"Building the skill network graph. This may take a moment...\")\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add all skills as nodes to the graph\n",
    "for skill in df.index:\n",
    "    G.add_node(skill)\n",
    "\n",
    "# Add edges between skills with their co-occurrence score as the 'weight'\n",
    "# We iterate through the upper triangle of the matrix to avoid adding duplicate edges\n",
    "for i in range(len(df.columns)):\n",
    "    for j in range(i + 1, len(df.columns)):\n",
    "        skill1 = df.columns[i]\n",
    "        skill2 = df.columns[j]\n",
    "        weight = df.loc[skill1, skill2]\n",
    "        \n",
    "        # Only add an edge if there is a connection (weight > 0)\n",
    "        if weight > 0:\n",
    "            G.add_edge(skill1, skill2, weight=weight)\n",
    "\n",
    "print(f\"Graph created successfully with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Calculating centrality measures...\")\n",
    "\n",
    "# --- Centrality Calculations ---\n",
    "\n",
    "# 1. Degree Centrality: How many direct connections a skill has.\n",
    "print(\"Calculating Degree Centrality...\")\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# 2. Betweenness Centrality: Identifies \"bridge\" skills.\n",
    "# NOTE: This is computationally intensive and can take a few minutes on a large graph.\n",
    "# We use 'weight' to consider the strength of connections. Since networkx treats higher weights\n",
    "# as 'longer distances', we need to invert our similarity scores.\n",
    "# We create a new attribute 'distance' which is 1/weight.\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if data['weight'] > 0:\n",
    "        data['distance'] = 1.0 / data['weight']\n",
    "    else:\n",
    "        data['distance'] = float('inf')\n",
    "\n",
    "print(\"Calculating Betweenness Centrality (this might take a while)...\")\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='distance', normalized=True)\n",
    "\n",
    "# 3. Eigenvector Centrality: Measures influence.\n",
    "print(\"Calculating Eigenvector Centrality...\")\n",
    "# This can sometimes fail to converge on complex graphs. We'll add error handling.\n",
    "try:\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, weight='weight', max_iter=1000)\n",
    "except nx.PowerIterationFailedConvergence:\n",
    "    print(\"Eigenvector centrality did not converge. Filling with 0.\")\n",
    "    eigenvector_centrality = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "print(\"Centrality calculations complete.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Assemble and Save the Results ---\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "centrality_df = pd.DataFrame({\n",
    "    \"Skill\": list(G.nodes),\n",
    "    \"Degree Centrality\": [degree_centrality.get(node, 0) for node in G.nodes()],\n",
    "    \"Betweenness Centrality\": [betweenness_centrality.get(node, 0) for node in G.nodes()],\n",
    "    \"Eigenvector Centrality\": [eigenvector_centrality.get(node, 0) for node in G.nodes()]\n",
    "})\n",
    "\n",
    "# Sort by a primary centrality measure for easier viewing\n",
    "centrality_df = centrality_df.sort_values(by=\"Degree Centrality\", ascending=False)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "centrality_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Successfully created '{OUTPUT_FILE}'.\")\n",
    "print(\"\\nHere's a preview of the top 10 most central skills:\")\n",
    "print(centrality_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a04386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_matrix.py\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv # Import the csv module\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_SKILLS_FILE = \"csv/skills_no_duplicate_sorted.csv\" # Using the cleaned, sorted skills file\n",
    "OUTPUT_MATRIX_FILE = \"correspendentFinalClean2.csv\"\n",
    "\n",
    "print(f\"Reading skills from '{INPUT_SKILLS_FILE}'...\")\n",
    "\n",
    "# Initialize a dictionary to store words and connections as sets to prevent duplicates\n",
    "words = defaultdict(set)\n",
    "\n",
    "# Define the chunk size for reading the CSV in parts\n",
    "chunk_size = 10000\n",
    "i = 0\n",
    "\n",
    "# Process CSV in chunks\n",
    "for chunk in pd.read_csv(INPUT_SKILLS_FILE, header=None, chunksize=chunk_size, low_memory=False):\n",
    "    i += 1\n",
    "    print(f\"Processing chunk {i}...\")\n",
    "    # Iterate over each row in the chunk\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        # Filter out NaN values and convert to list of strings\n",
    "        parts = [str(item) for item in row if pd.notna(item)]\n",
    "        # Generate pairwise combinations and update connections\n",
    "        for a, b in itertools.combinations(parts, 2):\n",
    "            if a and b:\n",
    "                words[a].add(b)\n",
    "                words[b].add(a)\n",
    "\n",
    "print(\"Finished processing skill pairs.\")\n",
    "\n",
    "# Convert keys to a list and determine the size of the final dataset\n",
    "keys = list(words.keys())\n",
    "size = len(keys)\n",
    "key_to_index = {key: i for i, key in enumerate(keys)} # Create a mapping for faster lookups\n",
    "\n",
    "print(f\"Found {size} unique skills. Building matrix...\")\n",
    "\n",
    "# Initialize a numpy array to track connections\n",
    "track = np.zeros((size, size))\n",
    "\n",
    "# Populate the track array based on the accumulated connections\n",
    "for i, k in enumerate(keys):\n",
    "    track[i, i] = len(words[k])  # Self-connection represents the degree of the node\n",
    "    for j in words[k]:\n",
    "        if j in key_to_index: # Ensure the skill is in our keys list\n",
    "            j_index = key_to_index[j]\n",
    "            track[i, j_index] += 1\n",
    "            # The matrix is symmetric, so we don't need to add to track[j_index, i] again\n",
    "\n",
    "print(\"Matrix built. Normalizing values...\")\n",
    "\n",
    "# Normalize each row in track by dividing each element by its diagonal entry\n",
    "# We need to handle division by zero for skills that have no connections (track[row,row] == 0)\n",
    "diagonal = track.diagonal().copy() # Make a copy to avoid modifying it while iterating\n",
    "diagonal[diagonal == 0] = 1 # Avoid division by zero, the result will be 0 anyway\n",
    "\n",
    "for row in range(track.shape[0]):\n",
    "    track[row,:] /= diagonal[row]\n",
    "\n",
    "print(\"Normalization complete. Saving to CSV...\")\n",
    "\n",
    "# Create a DataFrame from the track array with labels for rows and columns\n",
    "track_df = pd.DataFrame(track, index=keys, columns=keys)\n",
    "\n",
    "# Save the DataFrame to a CSV file. Pandas handles the quoting correctly.\n",
    "track_df.to_csv(OUTPUT_MATRIX_FILE)\n",
    "\n",
    "print(f\"Successfully created '{OUTPUT_MATRIX_FILE}'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c4832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_matrix.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Configuration ---\n",
    "# This is the file where each row represents a set of co-occurring skills for a project/job.\n",
    "INPUT_SKILLS_FILE = \"csv/skillsFreelancerFinal.csv\" \n",
    "\n",
    "# This will be the final, correctly formatted co-occurrence matrix.\n",
    "OUTPUT_MATRIX_FILE = \"skill_co_occurrence_matrix_with_duplicate.csv\"\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "print(f\"Step 1: Reading skills from '{INPUT_SKILLS_FILE}' and building co-occurrence map.\")\n",
    "\n",
    "# Use defaultdict(set) for efficient and duplicate-free storage of skill connections.\n",
    "skill_connections = defaultdict(set)\n",
    "all_skills = set()\n",
    "\n",
    "# We read the CSV line by line to handle varying numbers of skills per row.\n",
    "with open(INPUT_SKILLS_FILE, 'r', encoding='utf-8') as f:\n",
    "    # Skip the header row if it exists.\n",
    "    # If you are SURE there's no header, you can comment out the next line.\n",
    "    next(f, None) \n",
    "    \n",
    "    for line in f:\n",
    "        # Split by comma and strip whitespace/quotes from each skill\n",
    "        skills_in_row = [skill.strip().strip('\"') for skill in line.strip().split(',')]\n",
    "        \n",
    "        # Filter out any empty strings that might result from trailing commas\n",
    "        cleaned_skills = [skill for skill in skills_in_row if skill]\n",
    "        \n",
    "        # Add all unique skills to our master set\n",
    "        all_skills.update(cleaned_skills)\n",
    "        \n",
    "        # Generate all unique pairs of skills in this row\n",
    "        for skill1, skill2 in itertools.combinations(cleaned_skills, 2):\n",
    "            skill_connections[skill1].add(skill2)\n",
    "            skill_connections[skill2].add(skill1)\n",
    "\n",
    "print(f\"Step 2: Found {len(all_skills)} unique skills. Creating the co-occurrence matrix.\")\n",
    "\n",
    "# Create a sorted list of keys for consistent matrix ordering\n",
    "keys = sorted(list(all_skills))\n",
    "size = len(keys)\n",
    "\n",
    "# Create a mapping of skill_name -> index for much faster lookups\n",
    "key_to_index = {key: i for i, key in enumerate(keys)}\n",
    "\n",
    "# Initialize a numpy array to store the counts\n",
    "co_occurrence_counts = np.zeros((size, size), dtype=int)\n",
    "\n",
    "# Populate the matrix with co-occurrence counts\n",
    "for skill, connections in skill_connections.items():\n",
    "    i = key_to_index[skill]\n",
    "    # The diagonal will store the total number of connections (degree) for each skill\n",
    "    co_occurrence_counts[i, i] = len(connections)\n",
    "    for connected_skill in connections:\n",
    "        if connected_skill in key_to_index:\n",
    "            j = key_to_index[connected_skill]\n",
    "            # We simply count 1 for each co-occurrence\n",
    "            co_occurrence_counts[i, j] = 1\n",
    "\n",
    "print(\"Step 3: Normalizing the matrix to create correlation scores.\")\n",
    "\n",
    "# Create a copy of the matrix for normalization\n",
    "# We will divide each cell (i, j) by the total occurrences of skill i.\n",
    "# This gives P(j|i) - the probability of seeing skill j given that you see skill i.\n",
    "normalized_matrix = np.zeros((size, size), dtype=float)\n",
    "for i in range(size):\n",
    "    total_occurrences = co_occurrence_counts[i, i]\n",
    "    if total_occurrences > 0:\n",
    "        normalized_matrix[i, :] = co_occurrence_counts[i, :] / total_occurrences\n",
    "        normalized_matrix[i, i] = 1.0 # The probability of a skill co-occurring with itself is 1\n",
    "\n",
    "print(\"Step 4: Saving the final matrix to CSV.\")\n",
    "\n",
    "# Create a pandas DataFrame to save the result with proper headers and index\n",
    "final_df = pd.DataFrame(normalized_matrix, index=keys, columns=keys)\n",
    "\n",
    "# Save to CSV. Pandas will automatically handle quoting for skill names with commas.\n",
    "final_df.to_csv(OUTPUT_MATRIX_FILE)\n",
    "\n",
    "print(f\"\\nSuccess! Your skill intelligence matrix has been saved to '{OUTPUT_MATRIX_FILE}'.\")\n",
    "print(\"This file is the 'brain' of your application.\")\n",
    "print(\"\\nYou can now use this file as input for the 'generate_centrality.py' script and then the final 'cv_analyzer_app.py'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbfcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import collections\n",
    "\n",
    "# --- Configuration ---\n",
    "# This is your main co-occurrence matrix.\n",
    "# It should be the non-transposed version for easier handling here.\n",
    "# If you only have the transposed one, that's okay, this script will handle it.\n",
    "INPUT_MATRIX_FILE = 'skill_co_occurrence_matrix.csv' # Using the file from your screenshot\n",
    "OUTPUT_CLUSTERS_FILE = 'skill_clusters.csv'\n",
    "\n",
    "# This is the most important parameter to tune.\n",
    "# It defines the percentage of the WEAKEST skill connections to REMOVE before clustering.\n",
    "# A higher value (e.g., 0.90) will create more distinct, smaller clusters.\n",
    "# A lower value (e.g., 0.70) will create fewer, larger clusters.\n",
    "# Let's start high to ensure we break up that single giant cluster.\n",
    "PRUNING_QUANTILE = 0.95 # This means we will only keep the top 5% of strongest connections.\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "print(f\"Step 1: Loading the skill co-occurrence matrix from '{INPUT_MATRIX_FILE}'...\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_MATRIX_FILE, index_col=0)\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: The input file '{INPUT_MATRIX_FILE}' was not found.\")\n",
    "    print(\"Please make sure this script is in the same directory as your CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# Clean up names to be safe\n",
    "df.columns = df.columns.map(str)\n",
    "df.index = df.index.map(str)\n",
    "\n",
    "print(\"Step 2: Building the skill network graph from the matrix...\")\n",
    "# Create a graph from the pandas DataFrame\n",
    "G = nx.from_pandas_adjacency(df)\n",
    "print(f\"Initial graph created with {G.number_of_nodes()} skills and {G.number_of_edges()} connections.\")\n",
    "\n",
    "print(\"\\nStep 3: Pruning the graph to keep only the strongest connections...\")\n",
    "# Get all edge weights and determine the threshold for keeping the top connections\n",
    "all_weights = [data['weight'] for u, v, data in G.edges(data=True)]\n",
    "if not all_weights:\n",
    "    print(\"Error: No weights found in the graph. Cannot prune.\")\n",
    "    exit()\n",
    "\n",
    "threshold = pd.Series(all_weights).quantile(PRUNING_QUANTILE)\n",
    "print(f\"Calculated weight threshold: {threshold:.4f}. Edges with weight below this will be removed.\")\n",
    "\n",
    "# Create a new, pruned graph\n",
    "G_pruned = nx.Graph()\n",
    "# Add all skills first, so we don't lose any skills that might become isolated\n",
    "G_pruned.add_nodes_from(G) \n",
    "\n",
    "# Add only the edges that are above our threshold\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if data['weight'] >= threshold:\n",
    "        G_pruned.add_edge(u, v, weight=data['weight'])\n",
    "\n",
    "print(f\"Pruned graph has {G_pruned.number_of_nodes()} skills and {G_pruned.number_of_edges()} strong connections.\")\n",
    "\n",
    "print(\"\\nStep 4: Detecting skill communities using the Louvain algorithm...\")\n",
    "# Find communities (clusters) in the pruned graph\n",
    "# The 'seed' makes the result reproducible\n",
    "communities_sets = community.louvain_communities(G_pruned, weight='weight', seed=42)\n",
    "communities_sets = sorted(communities_sets, key=len, reverse=True) # Sort by size\n",
    "\n",
    "print(f\"Successfully identified {len(communities_sets)} distinct skill clusters.\")\n",
    "\n",
    "print(\"\\nStep 5: Formatting and saving the clusters to 'skill_clusters.csv'...\")\n",
    "# Create a dictionary to map each skill to its cluster ID\n",
    "community_dict = {}\n",
    "for i, cluster in enumerate(communities_sets):\n",
    "    for skill in cluster:\n",
    "        community_dict[skill] = i\n",
    "\n",
    "# Convert to a DataFrame\n",
    "community_df = pd.DataFrame(community_dict.items(), columns=[\"Skill\", \"Cluster_ID\"])\n",
    "community_df = community_df.sort_values(by=\"Cluster_ID\")\n",
    "\n",
    "# Save the clusters to a new CSV file\n",
    "community_df.to_csv(OUTPUT_CLUSTERS_FILE, index=False)\n",
    "\n",
    "print(f\"\\nSuccess! New '{OUTPUT_CLUSTERS_FILE}' has been created.\")\n",
    "print(\"\\n--- Preview of the 5 Largest Clusters Found ---\")\n",
    "\n",
    "for i in range(min(5, len(communities_sets))):\n",
    "    cluster_skills = list(communities_sets[i])\n",
    "    print(f\"\\nCluster {i} (Size: {len(cluster_skills)} skills):\")\n",
    "    # Print up to the first 10 skills in the cluster for preview\n",
    "    print(\", \".join(cluster_skills[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
