# Generated by Selenium IDE
import csv
import re
from time import sleep

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

options = webdriver.ChromeOptions()  # options pour chrome webdriver
options.add_argument('--ignore-certificate-errors')
options.add_argument('--incognito')
options.add_argument("--start-maximized")
#options.add_argument("--headless")

job = "Javascript Developer"
driver = webdriver.Chrome("C:\\drivers\\chromedriver.exe",
                          chrome_options=options)
# driver = webdriver.Chrome()
vars = {}

driver.get("https://www.careerbuilder.com/")
driver.find_element(By.ID, "Keywords").click()
element = driver.find_element(By.ID, "Keywords")
actions = ActionChains(driver)
actions.double_click(element).perform()
driver.find_element(By.ID, "Keywords").click()
driver.find_element(By.ID, "Keywords").send_keys(job)
driver.find_element(By.ID, "Keywords").send_keys(Keys.ENTER)

f = open("jobs.csv", "a",
         newline='')
csv_writer = csv.writer(f)

page_number = 26
sleep(5)
driver.find_element(By.XPATH, "//div[3]/div/div[3]/a").click()
sleep(5)
try:
    for element in range(1, 10):
        for child_element in range(1, 26):
            sleep(3)
            # WebDriverWait(driver, 4000).until(expected_conditions.presence_of_element_located((By.CSS_SELECTOR,
            #                     ".new-job-collection:nth-child("+str(page_number)+") > .data-results-content-parent:nth-child("+str(child_element)+") > .data-results-content")))
            driver.find_element(By.CSS_SELECTOR,
                                ".new-job-collection:nth-child(" + str(
                                    page_number) + ") > .data-results-content-parent:nth-child(" + str(
                                    child_element) + ") > .data-results-content").click()
            sleep(4)
            page_source = driver.page_source
            page = BeautifulSoup(page_source, 'lxml')
            tags = page.find("div", {"id": "jdp_description"})
            list_insert = []
            list_insert.append(job)
            text = re.sub('[^a-zA-Z0-9.\d\s]', '', tags.text)
            text = text.replace("\u202f", "")
            list_insert.append(text.replace("\n", " "))
            csv_writer.writerow(list_insert)
            print(job)
            if (child_element == 25):
                page_number += 1
                driver.find_element(By.XPATH, "//div[3]/div/div[3]/a").click()
                sleep(5)
except:
    print("limit of pages scrapped"+str(page_number)+ job)
finally:
    driver.close()
