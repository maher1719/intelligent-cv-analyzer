{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cca5051c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting: Creating Master Skill List ---\n",
      "  - Processing 20020 rows from 'jobs/jobs_clean.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from jobs_clean.csv: 100%|██████████| 20020/20020 [00:00<00:00, 169003.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing 20020 rows from 'jobs/jobs_clean.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from jobs_clean.csv: 100%|██████████| 20020/20020 [00:00<00:00, 1141128.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing 9488 rows from 'resumes/resume_data.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from resume_data.csv: 100%|██████████| 9488/9488 [00:00<00:00, 38701.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing 9488 rows from 'resumes/resume_data.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from resume_data.csv: 100%|██████████| 9488/9488 [00:00<00:00, 37885.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing 9488 rows from 'resumes/Resume_dataset/resume_data.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from resume_data.csv: 100%|██████████| 9488/9488 [00:00<00:00, 32116.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing 1200 rows from 'resumes/resume_job_description/resume_dataset_1200.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from resume_dataset_1200.csv: 100%|██████████| 1200/1200 [00:00<00:00, 643874.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing 1068 rows from 'jobs/jobDescription/job_dataset.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from job_dataset.csv: 100%|██████████| 1068/1068 [00:00<00:00, 250098.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing 1068 rows from 'jobs/jobDescription/job_dataset.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from job_dataset.csv: 100%|██████████| 1068/1068 [00:00<00:00, 462759.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - WARNING: Could not process 'jobs/linkedin_job/linkedin_jobs_analysis.csv'. Error: Usecols do not match columns, columns expected but not found: ['skills']\n",
      "  - Processing 1000 rows from 'jobs/job_market/job_market_unemployment_trends.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from job_market_unemployment_trends.csv: 100%|██████████| 1000/1000 [00:00<00:00, 884128.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing 1990 rows from 'online_courses/coursera_specialization/coursera.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from coursera.csv: 100%|██████████| 1990/1990 [00:00<00:00, 117454.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Processing 5725 rows from 'online_courses/1500_courses/alison.csv'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from alison.csv: 100%|██████████| 5725/5725 [00:00<00:00, 487759.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  - Processing LinkedIn relational data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  -> Extracting from LinkedIn skills: 100%|██████████| 213768/213768 [00:00<00:00, 2863711.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Successfully processed LinkedIn relational skills.\n",
      "\n",
      "--- Finalizing Master Skill List ---\n",
      "\n",
      "✅ Success! Created 'master_skill_list_unique.csv' with a total of 16451 unique skills.\n",
      "\n",
      "This file is now your definitive dictionary of all skills from your datasets.\n",
      "\n",
      "--- NEXT STEPS ---\n",
      "1. Manually review 'master_skill_list.csv' for any obvious errors or junk data.\n",
      "2. Use this file as the basis for regenerating your 'skill_ontology.csv'.\n",
      "3. Use your raw data files to create the 'master_skill_sets.csv' for co-occurrence analysis.\n"
     ]
    }
   ],
   "source": [
    "# create_master_skill_list.py\n",
    "import pandas as pd\n",
    "import ast  # For safely evaluating string representations of lists\n",
    "import os\n",
    "from tqdm import tqdm # For a nice progress bar. Install with: pip install tqdm\n",
    "\n",
    "print(\"--- Starting: Creating Master Skill List ---\")\n",
    "\n",
    "# This set will store all unique skills found, automatically handling duplicates.\n",
    "master_skill_set = set()\n",
    "\n",
    "# A list of files and the columns that contain skill information.\n",
    "# Format: (filepath, column_name, format_type)\n",
    "# format_type can be 'list_string', 'comma_string', 'semicolon_string', 'direct'\n",
    "files_to_process = [\n",
    "    ('jobs/jobs_clean.csv', 'skills_norm', 'list_string'),\n",
    "    ('jobs/jobs_clean.csv', 'required_skills', 'comma_string'),\n",
    "    ('resumes/resume_data.csv', 'skills', 'list_string'),\n",
    "    ('resumes/resume_data.csv', 'skills', 'list_string'), # Handling potential duplicate paths\n",
    "    ('resumes/Resume_dataset/resume_data.csv', 'skills', 'list_string'), # Handling potential duplicate paths\n",
    "    ('resumes/resume_job_description/resume_dataset_1200.csv', 'Skills', 'comma_string'),\n",
    "    ('jobs/jobDescription/job_dataset.csv', 'Skills', 'semicolon_string'),\n",
    "    ('jobs/jobDescription/job_dataset.csv', 'Keywords', 'semicolon_string'),\n",
    "    ('jobs/linkedin_job/linkedin_jobs_analysis.csv', 'skills', 'comma_string'),\n",
    "    ('jobs/job_market/job_market_unemployment_trends.csv', 'in_demand_skills', 'comma_string'),\n",
    "    ('online_courses/coursera_specialization/coursera.csv', 'Skills', 'list_string'),\n",
    "    ('online_courses/1500_courses/alison.csv', 'Skills', 'comma_string')\n",
    "]\n",
    "\n",
    "def clean_skill(skill):\n",
    "    \"\"\"Standardizes a single skill string.\"\"\"\n",
    "    if not isinstance(skill, str):\n",
    "        return None\n",
    "    s = skill.lower().strip()\n",
    "    # Remove extra characters that might be left over from parsing\n",
    "    s = s.replace(\"'\", \"\").replace('\"', \"\").replace('[', '').replace(']', '')\n",
    "    # Filter out junk values\n",
    "    if len(s) > 1 and len(s) < 50:\n",
    "        return s\n",
    "    return None\n",
    "\n",
    "def process_file(filepath, column, format_type, skill_set):\n",
    "    \"\"\"Reads a CSV, extracts skills from the specified column, and adds them to the master set.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"  - Skipping: File not found at '{filepath}'\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Use a more robust CSV reader for potential encoding issues\n",
    "        df = pd.read_csv(filepath, usecols=[column], encoding='utf-8', on_bad_lines='skip')\n",
    "        df.dropna(subset=[column], inplace=True)\n",
    "        \n",
    "        print(f\"  - Processing {len(df)} rows from '{filepath}'...\")\n",
    "\n",
    "        for item in tqdm(df[column], desc=f\"  -> Extracting from {os.path.basename(filepath)}\"):\n",
    "            skills_to_add = []\n",
    "            \n",
    "            # --- Handle different data formats ---\n",
    "            if format_type == 'list_string':\n",
    "                try:\n",
    "                    skills_to_add = ast.literal_eval(item)\n",
    "                except (ValueError, SyntaxError):\n",
    "                    # If it's not a perfect list string, treat it as a comma-separated string\n",
    "                    skills_to_add = str(item).split(',')\n",
    "            \n",
    "            elif format_type == 'comma_string':\n",
    "                skills_to_add = str(item).split(',')\n",
    "                \n",
    "            elif format_type == 'semicolon_string':\n",
    "                skills_to_add = str(item).split(';')\n",
    "\n",
    "            elif format_type == 'direct':\n",
    "                skills_to_add = [item]\n",
    "            \n",
    "            # --- Clean and add the extracted skills to the master set ---\n",
    "            if isinstance(skills_to_add, list):\n",
    "                for skill_str in skills_to_add:\n",
    "                    cleaned = clean_skill(skill_str)\n",
    "                    if cleaned:\n",
    "                        skill_set.add(cleaned)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  - WARNING: Could not process '{filepath}'. Error: {e}\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# Process all the standard files\n",
    "for filepath, column, format_type in files_to_process:\n",
    "    process_file(filepath, column, format_type, master_skill_set)\n",
    "\n",
    "# Special handling for the relational LinkedIn data\n",
    "print(\"\\n  - Processing LinkedIn relational data...\")\n",
    "try:\n",
    "    df_li_skills = pd.read_csv('jobs/linkedIn/jobs/job_skills.csv')\n",
    "    df_li_map = pd.read_csv('jobs/linkedIn/mappings/skills.csv')\n",
    "    df_li_merged = df_li_skills.merge(df_li_map, on='skill_abr', how='left')\n",
    "    df_li_merged.dropna(subset=['skill_name'], inplace=True)\n",
    "    \n",
    "    for skill in tqdm(df_li_merged['skill_name'], desc=\"  -> Extracting from LinkedIn skills\"):\n",
    "        cleaned = clean_skill(skill)\n",
    "        if cleaned:\n",
    "            master_skill_set.add(cleaned)\n",
    "    print(\"  - Successfully processed LinkedIn relational skills.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"  - Warning: Could not process LinkedIn relational data. File not found: {e.filename}\")\n",
    "\n",
    "\n",
    "# --- Finalize and Save ---\n",
    "print(\"\\n--- Finalizing Master Skill List ---\")\n",
    "\n",
    "# Convert the set to a sorted list for consistent ordering\n",
    "final_skill_list = sorted(list(master_skill_set))\n",
    "\n",
    "# Create a DataFrame\n",
    "df_master = pd.DataFrame(final_skill_list, columns=['skill'])\n",
    "\n",
    "# Save the final, clean, unique list of skills to a new CSV file\n",
    "output_filename = 'master_skill_list_unique.csv'\n",
    "df_master.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n✅ Success! Created '{output_filename}' with a total of {len(df_master)} unique skills.\")\n",
    "print(\"\\nThis file is now your definitive dictionary of all skills from your datasets.\")\n",
    "print(\"\\n--- NEXT STEPS ---\")\n",
    "print(\"1. Manually review 'master_skill_list.csv' for any obvious errors or junk data.\")\n",
    "print(\"2. Use this file as the basis for regenerating your 'skill_ontology.csv'.\")\n",
    "print(\"3. Use your raw data files to create the 'master_skill_sets.csv' for co-occurrence analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df46dc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting: Generating a unique skill list from 'master_skill_sets.csv' ---\n",
      "Reading and processing skill sets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 16it [00:02,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2723 unique skills.\n",
      "\n",
      "✅ Success! Saved the unique skills list to 'unique_skills.csv'.\n",
      "\n",
      "This file now serves as the master dictionary for your project.\n",
      "You can now proceed with generating the ontology and other analysis files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate_unique_skills.py\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILE = 'master_skill_sets.csv'\n",
    "OUTPUT_FILE = 'unique_skills.csv'\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Reads a CSV file where each row contains a set of skills,\n",
    "    and creates a new CSV file with a single column of all unique skills,\n",
    "    cleaned and sorted alphabetically.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting: Generating a unique skill list from '{INPUT_FILE}' ---\")\n",
    "\n",
    "    # 1. Check if the input file exists\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"ERROR: Input file '{INPUT_FILE}' not found.\")\n",
    "        print(\"Please run the data fusion script first to create it.\")\n",
    "        return\n",
    "\n",
    "    # 2. Read the data and collect all skills into a set\n",
    "    print(\"Reading and processing skill sets...\")\n",
    "    unique_skills_set = set()\n",
    "\n",
    "    # Read the CSV in chunks for memory efficiency, especially if the file is very large\n",
    "    chunk_size = 10000\n",
    "    for chunk in tqdm(pd.read_csv(INPUT_FILE, header=None, chunksize=chunk_size, low_memory=False), desc=\"Processing chunks\"):\n",
    "        # The `stack()` method is a very efficient way to convert all columns into a single series\n",
    "        # It also automatically drops empty/NaN values.\n",
    "        all_skills_in_chunk = chunk.stack()\n",
    "        \n",
    "        # Clean and add to our master set\n",
    "        for skill in all_skills_in_chunk:\n",
    "            # Clean up the skill: convert to string, lowercase, and strip whitespace\n",
    "            cleaned_skill = str(skill).strip().lower()\n",
    "            if cleaned_skill: # Ensure it's not an empty string\n",
    "                unique_skills_set.add(cleaned_skill)\n",
    "\n",
    "    print(f\"Found {len(unique_skills_set)} unique skills.\")\n",
    "\n",
    "    # 3. Convert the set to a sorted list\n",
    "    sorted_unique_skills = sorted(list(unique_skills_set))\n",
    "\n",
    "    # 4. Create a DataFrame and save to a new CSV\n",
    "    df_output = pd.DataFrame(sorted_unique_skills, columns=['skill'])\n",
    "    \n",
    "    df_output.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(f\"\\n✅ Success! Saved the unique skills list to '{OUTPUT_FILE}'.\")\n",
    "    print(\"\\nThis file now serves as the master dictionary for your project.\")\n",
    "    print(\"You can now proceed with generating the ontology and other analysis files.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7df1bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758643652.735454 1225148 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Google Gemini API.\n",
      "Reading base skill ontology from 'unique_skills.csv'...\n",
      "Found 2723 skills to process.\n",
      "Processing 2723 skills in batches of 40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Skill Knowledge Base: 100%|██████████| 69/69 [14:10<00:00, 12.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregation complete. Formatting final CSV.\n",
      "\n",
      "--- Success! ---\n",
      "Created 'skill_knowledge_base2.csv' with 1173 canonical skills.\n",
      "This file is now your central source of truth for skill semantics.\n",
      "\n",
      "Preview:\n",
      "  canonical_skill  skill_type              parent_skill  \\\n",
      "0            .net   framework      software development   \n",
      "2     3d graphics       field         computer graphics   \n",
      "3     3d modeling   technique               3d graphics   \n",
      "4     3d printing  technology  manufacturing technology   \n",
      "5         3ds max    software               3d modeling   \n",
      "\n",
      "                                         description       synonyms  \n",
      "0  A free, cross-platform, open-source developer ...  .ney(vb, asp)  \n",
      "2  The creation and manipulation of three-dimensi...             3d  \n",
      "3  The process of creating a mathematical represe...                 \n",
      "4  A process of creating a three-dimensional obje...                 \n",
      "5  A professional 3D computer graphics program fo...         3dsmax  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate_knowledge_base.py\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm # A library for progress bars, install with: pip install tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# IMPORTANT: Put your Google AI API Key here\n",
    "API_KEY = \"AIzaSyAD30hzssF3ozDEM1RFi97bVHJQNBMQSfY\"\n",
    "\n",
    "# Input file generated by your previous script\n",
    "INPUT_ONTOLOGY_FILE = 'unique_skills.csv'\n",
    "\n",
    "# The final, powerful output file\n",
    "OUTPUT_KB_FILE = 'skill_knowledge_base2.csv'\n",
    "\n",
    "# How many skills to process in each API call\n",
    "BATCH_SIZE = 40 # Smaller batch size for this more complex task\n",
    "\n",
    "# --- SCRIPT LOGIC ---\n",
    "\n",
    "def initialize_gemini():\n",
    "    \"\"\"Initializes the Gemini API and returns the model object.\"\"\"\n",
    "    try:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "        model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
    "        model.generate_content(\"test\", generation_config={\"max_output_tokens\": 10})\n",
    "        print(\"Successfully connected to Google Gemini API.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Gemini API: {e}\\nPlease check your API_KEY.\")\n",
    "        return None\n",
    "\n",
    "def generate_semantic_info(model, skill_batch):\n",
    "    \"\"\"Sends a batch of skills to the Gemini API to get synonyms and descriptions.\"\"\"\n",
    "    \n",
    "    skills_to_process = \", \".join([f'\"{skill}\"' for skill in skill_batch])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a data normalization and enrichment engine for a skills database. ignore completlely any unknown skill that isn't technical skill\n",
    "    For each of the following skills, perform two tasks:\n",
    "    1.  Provide a canonical, standardized name for the skill. For example, for \"js\" or \"reactjs\", the canonical name should be \"javascript\" or \"react\". For \"Python\", it should be \"python\".\n",
    "    2.  Provide a concise, one-sentence description of the skill.\n",
    "    3.  Provide parent skill\n",
    "    4.  Provide type of the content of the skill\n",
    "\n",
    "    Here is the list of skills: {skills_to_process}\n",
    "\n",
    "    Your output MUST be a valid JSON array of objects, with no other text before or after it. Each object must have three keys: \"original_skill\", \"canonical_name\", and \"description\".\n",
    "\n",
    "    Example output format:\n",
    "    [\n",
    "      {{\n",
    "        \"original_skill\": \"JS\",\n",
    "        \"canonical_name\": \"javascript\",\n",
    "        \"description\": \"A popular scripting language for web development.\",\n",
    "        \"parent_skill\":\"Web developement\",\n",
    "        \"skill_type\":\"Programming Language\"\n",
    "      }},\n",
    "      {{\n",
    "        \"original_skill\": \"amazon web services\",\n",
    "        \"canonical_name\": \"aws\",\n",
    "        \"description\": \"A comprehensive cloud computing platform by Amazon.\",\n",
    "        \"parent_skill\":\"cloud computing\",\n",
    "        \"skill_type\":\"Platform\"\n",
    "      }}\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt, generation_config={\"temperature\": 0.0})\n",
    "        # Clean the response to ensure it's valid JSON\n",
    "        clean_response = response.text.strip().replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        return json.loads(clean_response)\n",
    "    except (json.JSONDecodeError, Exception) as e:\n",
    "        print(f\"  -- API/JSON Error for a batch. Skipping. Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    if API_KEY == \"YOUR_API_KEY_HERE\":\n",
    "        print(\"ERROR: Please update the API_KEY variable in the script.\")\n",
    "        return\n",
    "\n",
    "    model = initialize_gemini()\n",
    "    if not model:\n",
    "        return\n",
    "\n",
    "    print(f\"Reading base skill ontology from '{INPUT_ONTOLOGY_FILE}'...\")\n",
    "    try:\n",
    "        df_ontology = pd.read_csv(INPUT_ONTOLOGY_FILE)\n",
    "        df_ontology['Skill'] = df_ontology['Skill'].str.lower()\n",
    "        # Set index for easier lookup later\n",
    "        df_ontology.set_index('Skill', inplace=True)\n",
    "        all_skills = df_ontology.index.tolist()\n",
    "        print(f\"Found {len(all_skills)} skills to process.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Cannot find the source file '{INPUT_ONTOLOGY_FILE}'. Please run generate_ontology.py first.\")\n",
    "        return\n",
    "        \n",
    "    # This dictionary will store our enriched data, keyed by the canonical name\n",
    "    knowledge_base = {}\n",
    "\n",
    "    print(f\"Processing {len(all_skills)} skills in batches of {BATCH_SIZE}...\")\n",
    "    \n",
    "    # Using tqdm for a nice progress bar in the terminal\n",
    "    for i in tqdm(range(0, len(all_skills), BATCH_SIZE), desc=\"Generating Skill Knowledge Base\"):\n",
    "        batch = all_skills[i:i + BATCH_SIZE]\n",
    "        \n",
    "        # Call the Gemini API for the current batch\n",
    "        semantic_data = generate_semantic_info(model, batch)\n",
    "        \n",
    "        # Process the results from the API\n",
    "        for item in semantic_data:\n",
    "            original = item.get('original_skill', '').lower().strip()\n",
    "            canonical = item.get('canonical_name', '').lower().strip()\n",
    "            description = item.get('description', 'No description available.')\n",
    "            skill_type= item.get('skill_type', '').lower().strip()\n",
    "            parent_skill= item.get('parent_skill', '').lower().strip()\n",
    "\n",
    "            \n",
    "            if not canonical: continue # Skip if AI failed to provide a canonical name\n",
    "\n",
    "            # If this is the first time we see this canonical name, create an entry\n",
    "            if canonical not in knowledge_base:\n",
    "\n",
    "                knowledge_base[canonical] = {\n",
    "                    'skill_type': skill_type,\n",
    "                    'parent_skill': parent_skill,\n",
    "                    'description': description,\n",
    "                    'synonyms': set()\n",
    "                }\n",
    "\n",
    "            # Add the original skill name as a synonym\n",
    "            if original != canonical:\n",
    "                knowledge_base[canonical]['synonyms'].add(original)\n",
    "\n",
    "        time.sleep(2) # Be respectful of API rate limits\n",
    "\n",
    "    print(\"\\nAggregation complete. Formatting final CSV.\")\n",
    "    \n",
    "    # Convert the dictionary to a list of records for the DataFrame\n",
    "    final_data = []\n",
    "    for canonical, data in knowledge_base.items():\n",
    "        synonyms_str = \", \".join(sorted(list(data['synonyms'])))\n",
    "        final_data.append({\n",
    "            'canonical_skill': canonical,\n",
    "            'skill_type': data['skill_type'],\n",
    "            'parent_skill': data['parent_skill'],\n",
    "            'description': data['description'],\n",
    "            'synonyms': synonyms_str\n",
    "        })\n",
    "        \n",
    "    # Create and save the final DataFrame\n",
    "    df_kb = pd.DataFrame(final_data)\n",
    "    df_kb.sort_values('canonical_skill', inplace=True)\n",
    "    df_kb.to_csv(OUTPUT_KB_FILE, index=False)\n",
    "    \n",
    "    print(f\"\\n--- Success! ---\")\n",
    "    print(f\"Created '{OUTPUT_KB_FILE}' with {len(df_kb)} canonical skills.\")\n",
    "    print(\"This file is now your central source of truth for skill semantics.\")\n",
    "    print(\"\\nPreview:\")\n",
    "    print(df_kb.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94281083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64 CSV files. Inspecting each one...\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/courses_usage/online_courses_uses new.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/courses_usage/online_courses_uses new.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/udacity.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/udacity.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/MIT ocw.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/MIT ocw.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/edx.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/edx.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/Oxford.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/Oxford.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/futurelearn.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/futurelearn.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/Stanford.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/Stanford.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/coursera_update.csv\n",
      "--------------------------------------------------\n",
      "Could not read or process file. Error: 'utf-8' codec can't decode byte 0xc9 in position 77: invalid continuation byte\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/swayam.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/swayam.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/london school of economics.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/london school of economics.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/Barkeley_extension.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/Barkeley_extension.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/Harvard_university.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/Harvard_university.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/pluralsight.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/pluralsight.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/1500_courses/alison.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/1500_courses/alison.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: online_courses/coursera_specialization/coursera.csv\n",
      "--------------------------------------------------\n",
      "Results for online_courses/coursera_specialization/coursera.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: resumes/resume_data.csv\n",
      "--------------------------------------------------\n",
      "Results for resumes/resume_data.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: resumes/resume_data (1).csv\n",
      "--------------------------------------------------\n",
      "Results for resumes/resume_data (1).csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: resumes/Resume_dataset/resume_data.csv\n",
      "--------------------------------------------------\n",
      "Results for resumes/Resume_dataset/resume_data.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: resumes/resume_job_description/resume_dataset_1200.csv\n",
      "--------------------------------------------------\n",
      "Results for resumes/resume_job_description/resume_dataset_1200.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: resumes/Education_career_success/education_career_success.csv\n",
      "--------------------------------------------------\n",
      "Results for resumes/Education_career_success/education_career_success.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/Bibliometric Job Search.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/Bibliometric Job Search.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/jobs_clean.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/jobs_clean.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/700jobs/jobs_dataset.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/700jobs/jobs_dataset.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/jobDescription/job_dataset.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/jobDescription/job_dataset.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/job_market/job_market_unemployment_trends.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/job_market/job_market_unemployment_trends.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedin_job/linkedin_jobs_analysis.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedin_job/linkedin_jobs_analysis.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/sector-job-title-examples.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/sector-job-title-examples.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/CA/job_postings_by_sector_CA.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/CA/job_postings_by_sector_CA.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/CA/provincial_postings_ca.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/CA/provincial_postings_ca.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/CA/aggregate_job_postings_CA.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/CA/aggregate_job_postings_CA.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/CA/metro_job_postings_CA.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/CA/metro_job_postings_CA.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/IE/aggregate_job_postings_IE.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/IE/aggregate_job_postings_IE.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/IT/aggregate_job_postings_IT.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/IT/aggregate_job_postings_IT.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/EA/aggregate_job_postings_EA.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/EA/aggregate_job_postings_EA.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/FR/job_postings_by_sector_FR.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/FR/job_postings_by_sector_FR.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/FR/aggregate_job_postings_FR.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/FR/aggregate_job_postings_FR.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/ES/aggregate_job_postings_ES.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/ES/aggregate_job_postings_ES.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/AU/aggregate_job_postings_AU.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/AU/aggregate_job_postings_AU.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/AU/job_postings_by_sector_AU.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/AU/job_postings_by_sector_AU.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/DE/job_postings_by_sector_DE.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/DE/job_postings_by_sector_DE.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/DE/aggregate_job_postings_DE.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/DE/aggregate_job_postings_DE.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/NL/aggregate_job_postings_NL.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/NL/aggregate_job_postings_NL.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/US/metro_job_postings_us.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/US/metro_job_postings_us.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/US/aggregate_job_postings_US.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/US/aggregate_job_postings_US.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/US/state_job_postings_us.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/US/state_job_postings_us.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/US/job_postings_by_sector_US.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/US/job_postings_by_sector_US.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/GB/regional_gb.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/GB/regional_gb.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/GB/job_postings_by_sector_GB.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/GB/job_postings_by_sector_GB.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/GB/city_postings_gb.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/GB/city_postings_gb.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_jobs/GB/aggregate_job_postings_GB.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_jobs/GB/aggregate_job_postings_GB.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/indeed_companies/companies_data.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/indeed_companies/companies_data.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/final_job_data/final_job.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/final_job_data/final_job.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/postings.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/postings.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/mappings/skills.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/mappings/skills.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/mappings/industries.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/mappings/industries.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/jobs/job_skills.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/jobs/job_skills.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/jobs/salaries.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/jobs/salaries.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/jobs/job_industries.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/jobs/job_industries.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/jobs/benefits.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/jobs/benefits.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/companies/companies.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/companies/companies.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/companies/employee_counts.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/companies/employee_counts.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/companies/company_specialities.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/companies/company_specialities.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/linkedIn/companies/company_industries.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/linkedIn/companies/company_industries.csv written to csv_inspection_results.txt\n",
      "\n",
      "--------------------------------------------------\n",
      "File: jobs/archive_Job/job_title_des.csv\n",
      "--------------------------------------------------\n",
      "Results for jobs/archive_Job/job_title_des.csv written to csv_inspection_results.txt\n",
      "\n",
      "All results saved to csv_inspection_results.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Find all CSV files in the current directory and all subdirectories\n",
    "csv_files = glob.glob('**/*.csv', recursive=True)\n",
    "\n",
    "output_txt = 'csv_inspection_results.txt'  # Name of the output file\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found. Please make sure your CSV files are in the working directory or its subdirectories.\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_files)} CSV files. Inspecting each one...\\n\")\n",
    "\n",
    "    # Open the output file once\n",
    "    with open(output_txt, 'w') as f:\n",
    "        f.write(f\"CSV Inspection Report - Found {len(csv_files)} CSV files\\n\\n\")\n",
    "\n",
    "        for file_path in csv_files:\n",
    "            print(\"--------------------------------------------------\")\n",
    "            print(f\"File: {file_path}\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "            try:\n",
    "                # Read the CSV into a pandas DataFrame\n",
    "                df = pd.read_csv(file_path, on_bad_lines='skip')\n",
    "\n",
    "                # Write results to the output file\n",
    "                f.write(f\"File: {file_path}\\n\")\n",
    "                f.write(f\"Shape: {df.shape}\\n\\n\")\n",
    "                f.write(\"Head:\\n\")\n",
    "                f.write(df.head().to_string())\n",
    "                f.write(\"\\n\\n\")\n",
    "\n",
    "                print(f\"Results for {file_path} written to {output_txt}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                f.write(f\"File: {file_path}\\n\")\n",
    "                f.write(f\"Error: {e}\\n\\n\")\n",
    "                print(f\"Could not read or process file. Error: {e}\\n\")\n",
    "\n",
    "    print(f\"All results saved to {output_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab57f6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Fusion and Preparation ---\n",
      "\n",
      "[Phase 1/3] Extracting skill sets from all data sources...\n",
      "  - Successfully processed resumes/resume_data.csv\n",
      "  - Successfully processed jobs_clean.csv\n",
      "  - Successfully processed LinkedIn job skills data.\n",
      "\n",
      "[Phase 2/3] Generating Master Skill Files...\n",
      "  - Created master_skill_list.csv with 2723 unique skills.\n",
      "  - Extracting skills from raw text descriptions (this may take a minute)...\n",
      "    - Processed jobs/archive_Job/job_title_des.csv\n",
      "  - Created master_skill_sets.csv with 158517 total entries.\n",
      "\n",
      "[Phase 3/3] Generating Role Definitions...\n",
      "  - Created role_definitions.csv with 50 common job archetypes.\n",
      "\n",
      "--- DATA FUSION COMPLETE ---\n",
      "You should now re-run your analysis pipeline (generate_matrix, generate_centrality, etc.) using 'master_skill_sets.csv' as the input.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 163\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Error creating role definitions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Skipping this step.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[43mmain\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "# data_fusion.py\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "print(\"--- Starting Data Fusion and Preparation ---\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: EXTRACT SKILL SETS FROM ALL SOURCES\n",
    "# ==============================================================================\n",
    "print(\"\\n[Phase 1/3] Extracting skill sets from all data sources...\")\n",
    "\n",
    "all_skill_sets = []\n",
    "master_skill_set = set()\n",
    "\n",
    "def clean_and_add_skills(skill_list):\n",
    "    \"\"\"Helper function to clean, normalize, and add skills to the master set and list.\"\"\"\n",
    "    if not isinstance(skill_list, list):\n",
    "        return None\n",
    "    \n",
    "    cleaned_list = [str(s).strip().lower() for s in skill_list if pd.notna(s) and str(s).strip()]\n",
    "    \n",
    "    # Simple rule to filter out junk/long phrases\n",
    "    final_skills = [s for s in cleaned_list if len(s) < 50 and len(s) > 1]\n",
    "    \n",
    "    if final_skills:\n",
    "        master_skill_set.update(final_skills)\n",
    "        all_skill_sets.append(final_skills)\n",
    "\n",
    "# Source 1: resume_data.csv\n",
    "try:\n",
    "    df_resume = pd.read_csv('resumes/resume_data.csv', usecols=['skills'])\n",
    "    df_resume.dropna(subset=['skills'], inplace=True)\n",
    "    df_resume['skills'].apply(lambda x: clean_and_add_skills(ast.literal_eval(x)))\n",
    "    print(\"  - Successfully processed resumes/resume_data.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"  - Warning: Could not process resumes/resume_data.csv. Error: {e}\")\n",
    "\n",
    "# Source 2: jobs_clean.csv\n",
    "try:\n",
    "    df_jobs_clean = pd.read_csv('jobs/jobs_clean.csv', usecols=['skills_norm'])\n",
    "    df_jobs_clean.dropna(subset=['skills_norm'], inplace=True)\n",
    "    df_jobs_clean['skills_norm'].apply(lambda x: clean_and_add_skills(ast.literal_eval(x)))\n",
    "    print(\"  - Successfully processed jobs_clean.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"  - Warning: Could not process jobs_clean.csv. Error: {e}\")\n",
    "\n",
    "# Source 3: LinkedIn Data\n",
    "try:\n",
    "    df_li_jobs = pd.read_csv('jobs/linkedIn/jobs/job_skills.csv')\n",
    "    df_li_map = pd.read_csv('jobs/linkedIn/mappings/skills.csv').rename(columns={'skill_name': 'skill'})\n",
    "    df_li_merged = df_li_jobs.merge(df_li_map, on='skill_abr', how='left')\n",
    "    \n",
    "    # Group by job_id to create skill lists\n",
    "    linkedin_skill_lists = df_li_merged.groupby('job_id')['skill'].apply(list).tolist()\n",
    "    for skill_list in linkedin_skill_lists:\n",
    "        clean_and_add_skills(skill_list)\n",
    "    print(\"  - Successfully processed LinkedIn job skills data.\")\n",
    "except Exception as e:\n",
    "    print(f\"  - Warning: Could not process LinkedIn data. Error: {e}\")\n",
    "    \n",
    "# Source 4: Job Description Text (for keyword spotting)\n",
    "def extract_skills_from_text(text, known_skills):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text_lower = \" \" + text.lower() + \" \"\n",
    "    return [skill for skill in known_skills if f\" {skill} \" in text_lower]\n",
    "\n",
    "print(\"\\n[Phase 2/3] Generating Master Skill Files...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: CREATE master_skill_list.csv\n",
    "# ==============================================================================\n",
    "master_skill_list_sorted = sorted(list(master_skill_set))\n",
    "df_master_list = pd.DataFrame(master_skill_list_sorted, columns=['skill'])\n",
    "df_master_list.to_csv('master_skill_list.csv', index=False)\n",
    "print(f\"  - Created master_skill_list.csv with {len(df_master_list)} unique skills.\")\n",
    "\n",
    "# Now that we have a master list, we can extract from raw text descriptions\n",
    "print(\"  - Extracting skills from raw text descriptions (this may take a minute)...\")\n",
    "try:\n",
    "    df_archive = pd.read_csv('jobs/archive_Job/job_title_des.csv', usecols=['Job Description'])\n",
    "    df_archive.dropna(inplace=True)\n",
    "    df_archive['skills'] = df_archive['Job Description'].apply(lambda x: extract_skills_from_text(x, master_skill_set))\n",
    "    for skill_list in df_archive['skills'].tolist():\n",
    "        if skill_list: # Only add if skills were found\n",
    "            all_skill_sets.append(skill_list)\n",
    "    print(\"    - Processed jobs/archive_Job/job_title_des.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"  - Warning: Could not process jobs/archive_Job/job_title_des.csv. Error: {e}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 3: CREATE master_skill_sets.csv\n",
    "# ==============================================================================\n",
    "# This file is the foundation for your co-occurrence matrix.\n",
    "max_len = max(len(s) for s in all_skill_sets) if all_skill_sets else 0\n",
    "df_master_sets = pd.DataFrame(all_skill_sets) # Let pandas name columns 0, 1, 2...\n",
    "df_master_sets.to_csv('master_skill_sets.csv', index=False, header=False) # No header needed for the next script\n",
    "print(f\"  - Created master_skill_sets.csv with {len(df_master_sets)} total entries.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 4: CREATE role_definitions.csv (by analyzing common job titles)\n",
    "# ==============================================================================\n",
    "print(\"\\n[Phase 3/3] Generating Role Definitions...\")\n",
    "\n",
    "try:\n",
    "    # Use the most descriptive job title columns\n",
    "    df_jobs1 = pd.read_csv('jobs/jobs_clean.csv', usecols=['title', 'experience_level_raw', 'skills_norm'])\n",
    "    df_jobs2 = pd.read_csv('jobs/700jobs/jobs_dataset.csv', usecols=['positionName', 'description'])\n",
    "    df_jobs2.rename(columns={'positionName': 'title'}, inplace=True)\n",
    "    df_jobs2['experience_level_raw'] = 'Not Specified' # Add column for consistency\n",
    "    df_jobs2['skills_norm'] = df_jobs2['description'].apply(lambda x: str(extract_skills_from_text(x, master_skill_set)))\n",
    "\n",
    "    # Combine the datasets\n",
    "    df_all_jobs = pd.concat([df_jobs1[['title', 'experience_level_raw', 'skills_norm']],\n",
    "                             df_jobs2[['title', 'experience_level_raw', 'skills_norm']]], \n",
    "                            ignore_index=True)\n",
    "    \n",
    "    df_all_jobs.dropna(subset=['title', 'skills_norm'], inplace=True)\n",
    "\n",
    "    # Normalize job titles\n",
    "    df_all_jobs['title_norm'] = df_all_jobs['title'].str.lower().str.strip()\n",
    "    \n",
    "    # Aggregate skills for the top 50 most common job titles\n",
    "    top_titles = df_all_jobs['title_norm'].value_counts().nlargest(50).index\n",
    "    \n",
    "    role_definitions = []\n",
    "    \n",
    "    for title in top_titles:\n",
    "        subset_df = df_all_jobs[df_all_jobs['title_norm'] == title]\n",
    "        \n",
    "        # Aggregate all skills for this title\n",
    "        all_skills_for_title = []\n",
    "        subset_df['skills_norm'].apply(lambda x: all_skill_sets.append(ast.literal_eval(x)))\n",
    "        flat_list = [item for sublist in subset_df['skills_norm'].apply(ast.literal_eval).tolist() for item in sublist]\n",
    "        \n",
    "        # Find the 5 most common skills for this role\n",
    "        core_skills = [skill for skill, count in Counter(flat_list).most_common(5)]\n",
    "        \n",
    "        # Determine the most common experience level\n",
    "        experience_level = subset_df['experience_level_raw'].mode().iloc[0] if not subset_df['experience_level_raw'].mode().empty else 'Various'\n",
    "        \n",
    "        role_definitions.append({\n",
    "            'role_title': title.title(),\n",
    "            'core_skills': \", \".join(core_skills),\n",
    "            'typical_experience_level': experience_level\n",
    "        })\n",
    "\n",
    "    df_roles = pd.DataFrame(role_definitions)\n",
    "    df_roles.to_csv('role_definitions.csv', index=False)\n",
    "    print(f\"  - Created role_definitions.csv with {len(df_roles)} common job archetypes.\")\n",
    "    print(\"\\n--- DATA FUSION COMPLETE ---\")\n",
    "    print(\"You should now re-run your analysis pipeline (generate_matrix, generate_centrality, etc.) using 'master_skill_sets.csv' as the input.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  - Error creating role definitions: {e}. Skipping this step.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db95d7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Enhancement Pipeline ---\n",
      "\n",
      "[1/3] Generating Skill Seniority Profiles...\n",
      "  - Success! Saved seniority profiles for 40 skills to 'skill_seniority_profile.csv'.\n",
      "  - Preview:\n",
      "level    pct_junior    pct_mid  pct_senior primary_level\n",
      "skill                                                   \n",
      "angular   31.884058  34.492754   33.623188           mid\n",
      "aws       34.807554  32.464738   32.727707        junior\n",
      "azure     34.006491  32.311576   33.681933        junior\n",
      "c#        34.899329  36.241611   28.859060           mid\n",
      "css       35.636856  35.501355   28.861789        junior\n",
      "\n",
      "[2/3] Generating Skill Industry Focus...\n",
      "  - Success! Saved top skills for 388 industries to 'skill_industry_focus.csv'.\n",
      "  - Preview:\n",
      "                                      industry_name  \\\n",
      "0  Abrasives and Nonmetallic Minerals Manufacturing   \n",
      "1                Accessible Architecture and Design   \n",
      "2                   Accommodation and Food Services   \n",
      "3                                        Accounting   \n",
      "4                         Administration of Justice   \n",
      "\n",
      "                                          top_skills  \n",
      "0                                        Engineering  \n",
      "1  Design, Engineering, Project Management, Infor...  \n",
      "2  Customer Service, Analyst, Administrative, Bus...  \n",
      "3  Accounting/Auditing, Finance, Information Tech...  \n",
      "4  Legal, Administrative, Strategy/Planning, Cust...  \n",
      "\n",
      "[3/3] Generating Skill Demand Trends...\n",
      "  - Success! Saved demand trends for 40 skills to 'skill_demand_trends.csv'.\n",
      "  - Preview of trending up skills:\n",
      "                Skill  Demand_Change_6M_Pct\n",
      "16            jenkins                   inf\n",
      "22            next.js                   inf\n",
      "28            pytorch                   inf\n",
      "29  quantum computing                   inf\n",
      "38         typescript                   inf\n",
      "\n",
      "  - Preview of trending down skills:\n",
      "            Skill  Demand_Change_6M_Pct\n",
      "8   generative ai                -100.0\n",
      "10             go                -100.0\n",
      "19  microservices                -100.0\n",
      "31          redux                -100.0\n",
      "37      terraform                -100.0\n",
      "\n",
      "--- All enhancement files created successfully! ---\n",
      "You can now integrate these into your application's logic.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"--- Starting Data Enhancement Pipeline ---\")\n",
    "\n",
    "\n",
    "def generate_seniority_profiles():\n",
    "    \"\"\"\n",
    "    Generates a CSV file mapping skills to their seniority profiles (junior, mid, senior).\n",
    "    \"\"\"\n",
    "    print(\"\\n[1/3] Generating Skill Seniority Profiles...\")\n",
    "    try:\n",
    "        df = pd.read_csv('jobs/jobs_clean.csv', usecols=['experience_level_raw', 'skills_norm'])\n",
    "        df.dropna(subset=['experience_level_raw', 'skills_norm'], inplace=True)\n",
    "    except FileNotFoundError:\n",
    "        print(\" - ERROR: 'jobs_clean.csv' not found. Cannot generate seniority profiles.\")\n",
    "        return\n",
    "\n",
    "    # Normalize the experience levels into three categories\n",
    "    def map_level(level):\n",
    "        level = str(level).lower()\n",
    "        if 'senior' in level or 'lead' in level or 'expert' in level:\n",
    "            return 'senior'\n",
    "        if 'junior' in level or 'entry' in level or 'fresher' in level:\n",
    "            return 'junior'\n",
    "        if 'mid' in level or 'intermediate' in level:\n",
    "            return 'mid'\n",
    "        return 'other'\n",
    "\n",
    "    df['level'] = df['experience_level_raw'].apply(map_level)\n",
    "    df = df[df['level'] != 'other']  # We only care about jobs with clear seniority\n",
    "\n",
    "    # Explode the skills into a long format\n",
    "    df['skills_norm'] = df['skills_norm'].apply(ast.literal_eval)\n",
    "    df_exploded = df.explode('skills_norm').rename(columns={'skills_norm': 'skill'})\n",
    "\n",
    "    # Count skill occurrences for each level\n",
    "    skill_level_counts = df_exploded.groupby(['skill', 'level']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate percentages\n",
    "    total_counts = skill_level_counts.sum(axis=1)\n",
    "    skill_level_pct = skill_level_counts.div(total_counts, axis=0) * 100\n",
    "\n",
    "    # Identify the primary level for each skill\n",
    "    skill_level_pct['primary_level'] = skill_level_pct.idxmax(axis=1)\n",
    "    skill_level_pct.rename(columns={'junior': 'pct_junior', 'mid': 'pct_mid', 'senior': 'pct_senior'}, inplace=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = 'skill_seniority_profile.csv'\n",
    "    skill_level_pct.to_csv(output_file)\n",
    "    print(f\"  - Success! Saved seniority profiles for {len(skill_level_pct)} skills to '{output_file}'.\")\n",
    "    print(\"  - Preview:\")\n",
    "    print(skill_level_pct.head())\n",
    "\n",
    "\n",
    "def generate_industry_focus():\n",
    "    \"\"\"\n",
    "    Generates a CSV file of the top 20 most frequent skills for each industry.\n",
    "    \"\"\"\n",
    "    print(\"\\n[2/3] Generating Skill Industry Focus...\")\n",
    "    try:\n",
    "        # Load the necessary LinkedIn relational files\n",
    "        df_job_skills = pd.read_csv('jobs/linkedIn/jobs/job_skills.csv')\n",
    "        df_skills_map = pd.read_csv('jobs/linkedIn/mappings/skills.csv').rename(columns={'skill_name': 'skill'})\n",
    "        df_job_industries = pd.read_csv('jobs/linkedIn/jobs/job_industries.csv')\n",
    "        df_industries_map = pd.read_csv('jobs/linkedIn/mappings/industries.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\" - ERROR: A required LinkedIn file was not found ({e.filename}). Cannot generate industry focus.\")\n",
    "        return\n",
    "\n",
    "    # Merge to get full names for skills and industries\n",
    "    skills_full = df_job_skills.merge(df_skills_map, on='skill_abr', how='left')\n",
    "    industries_full = df_job_industries.merge(df_industries_map, on='industry_id', how='left')\n",
    "\n",
    "    # Merge skills with industries on job_id\n",
    "    df_merged = skills_full.merge(industries_full, on='job_id', how='inner')\n",
    "    df_merged.dropna(subset=['skill', 'industry_name'], inplace=True)\n",
    "\n",
    "    # Count skills within each industry\n",
    "    industry_skill_counts = df_merged.groupby(['industry_name', 'skill']).size().reset_index(name='count')\n",
    "\n",
    "    # Find the top 20 skills for each industry\n",
    "    top_skills_per_industry = industry_skill_counts.groupby('industry_name').apply(\n",
    "        lambda x: x.nlargest(20, 'count')['skill'].tolist()\n",
    "    ).reset_index(name='top_skills')\n",
    "\n",
    "    # Convert list to a comma-separated string for easier CSV reading\n",
    "    top_skills_per_industry['top_skills'] = top_skills_per_industry['top_skills'].apply(lambda x: ', '.join(x))\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = 'skill_industry_focus.csv'\n",
    "    top_skills_per_industry.to_csv(output_file, index=False)\n",
    "    print(f\"  - Success! Saved top skills for {len(top_skills_per_industry)} industries to '{output_file}'.\")\n",
    "    print(\"  - Preview:\")\n",
    "    print(top_skills_per_industry.head())\n",
    "\n",
    "\n",
    "def generate_skill_trends():\n",
    "    \"\"\"\n",
    "    Analyzes and saves the 6-month demand trend for each skill.\n",
    "    \"\"\"\n",
    "    print(\"\\n[3/3] Generating Skill Demand Trends...\")\n",
    "    try:\n",
    "        df = pd.read_csv('jobs/jobs_clean.csv', usecols=['date_posted', 'skills_norm'])\n",
    "        df.dropna(subset=['date_posted', 'skills_norm'], inplace=True)\n",
    "    except FileNotFoundError:\n",
    "        print(\" - ERROR: 'jobs_clean.csv' not found. Cannot generate skill trends.\")\n",
    "        return\n",
    "\n",
    "    # Convert date and create a 'year_month' column for grouping\n",
    "    df['date_posted'] = pd.to_datetime(df['date_posted'], errors='coerce')\n",
    "    df.dropna(subset=['date_posted'], inplace=True)\n",
    "    df['period'] = df['date_posted'].dt.to_period('M')\n",
    "\n",
    "    # Explode skills\n",
    "    df['skills_norm'] = df['skills_norm'].apply(ast.literal_eval)\n",
    "    df_exploded = df.explode('skills_norm').rename(columns={'skills_norm': 'skill'})\n",
    "\n",
    "    # Count skills per period\n",
    "    trends = df_exploded.groupby(['period', 'skill']).size().reset_index(name='count')\n",
    "\n",
    "    # Pivot to get skills as columns and time as rows\n",
    "    pivot_trends = trends.pivot(index='period', columns='skill', values='count').fillna(0)\n",
    "\n",
    "    # Calculate percentage change over 6 months\n",
    "    trend_data = pivot_trends.pct_change(periods=6).iloc[-1].reset_index(name='trend_6m')\n",
    "    trend_data['trend_6m'] = (trend_data['trend_6m'] * 100).round(2)  # As percentage\n",
    "    trend_data.rename(columns={'skill': 'Skill', 'trend_6m': 'Demand_Change_6M_Pct'}, inplace=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    output_file = 'skill_demand_trends.csv'\n",
    "    trend_data.to_csv(output_file, index=False)\n",
    "    print(f\"  - Success! Saved demand trends for {len(trend_data)} skills to '{output_file}'.\")\n",
    "    print(\"  - Preview of trending up skills:\")\n",
    "    print(trend_data.nlargest(5, 'Demand_Change_6M_Pct'))\n",
    "    print(\"\\n  - Preview of trending down skills:\")\n",
    "    print(trend_data.nsmallest(5, 'Demand_Change_6M_Pct'))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_seniority_profiles()\n",
    "    generate_industry_focus()\n",
    "    generate_skill_trends()\n",
    "    print(\"\\n--- All enhancement files created successfully! ---\")\n",
    "    print(\"You can now integrate these into your application's logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1184da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading skill relationship data from 'skill_co_occurrence_matrix.csv'...\n",
      "Building the skill network graph. This may take a moment...\n",
      "Graph created successfully with 2895 nodes and 523358 edges.\n",
      "------------------------------\n",
      "Calculating centrality measures...\n",
      "Calculating Degree Centrality...\n",
      "Calculating Betweenness Centrality (this might take a while)...\n",
      "Calculating Eigenvector Centrality...\n",
      "Centrality calculations complete.\n",
      "------------------------------\n",
      "Successfully created 'centrality_measures.csv'.\n",
      "\n",
      "Here's a preview of the top 10 most central skills:\n",
      "                       Skill  Degree Centrality  Betweenness Centrality  \\\n",
      "1611               Local Job           0.798549                0.085545   \n",
      "1915                     PHP           0.703179                0.042534   \n",
      "862               Data Entry           0.691431                0.011403   \n",
      "2402   Software Architecture           0.681755                0.093371   \n",
      "1282          Graphic Design           0.668970                0.008447   \n",
      "1050                   Excel           0.658950                0.017903   \n",
      "2754          Website Design           0.635453                0.042477   \n",
      "2132                  Python           0.633725                0.085393   \n",
      "1757  Mobile App Development           0.619558                0.025952   \n",
      "265                  Android           0.618867                0.000829   \n",
      "\n",
      "      Eigenvector Centrality  \n",
      "1611                0.083226  \n",
      "1915                0.093993  \n",
      "862                 0.047533  \n",
      "2402                0.144435  \n",
      "1282                0.046638  \n",
      "1050                0.045119  \n",
      "2754                0.090715  \n",
      "2132                0.267781  \n",
      "1757                0.074002  \n",
      "265                 0.022721  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# The input file is the co-occurrence matrix you showed me.\n",
    "INPUT_FILE = 'skill_co_occurrence_matrix.csv' \n",
    "OUTPUT_FILE = 'centrality_measures.csv'\n",
    "\n",
    "print(f\"Reading skill relationship data from '{INPUT_FILE}'...\")\n",
    "\n",
    "try:\n",
    "    # Load the co-occurrence matrix. The first column contains the skill names, so it becomes the index.\n",
    "    df = pd.read_csv(INPUT_FILE, index_col=0)\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: The input file '{INPUT_FILE}' was not found.\")\n",
    "    print(\"Please make sure this script is in the same directory as your CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# The networkx library works best with clean column names. Let's ensure they are all strings.\n",
    "df.columns = df.columns.astype(str)\n",
    "df.index = df.index.astype(str)\n",
    "\n",
    "print(\"Building the skill network graph. This may take a moment...\")\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add all skills as nodes to the graph\n",
    "for skill in df.index:\n",
    "    G.add_node(skill)\n",
    "\n",
    "# Add edges between skills with their co-occurrence score as the 'weight'\n",
    "# We iterate through the upper triangle of the matrix to avoid adding duplicate edges\n",
    "for i in range(len(df.columns)):\n",
    "    for j in range(i + 1, len(df.columns)):\n",
    "        skill1 = df.columns[i]\n",
    "        skill2 = df.columns[j]\n",
    "        weight = df.loc[skill1, skill2]\n",
    "        \n",
    "        # Only add an edge if there is a connection (weight > 0)\n",
    "        if weight > 0:\n",
    "            G.add_edge(skill1, skill2, weight=weight)\n",
    "\n",
    "print(f\"Graph created successfully with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Calculating centrality measures...\")\n",
    "\n",
    "# --- Centrality Calculations ---\n",
    "\n",
    "# 1. Degree Centrality: How many direct connections a skill has.\n",
    "print(\"Calculating Degree Centrality...\")\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "# 2. Betweenness Centrality: Identifies \"bridge\" skills.\n",
    "# NOTE: This is computationally intensive and can take a few minutes on a large graph.\n",
    "# We use 'weight' to consider the strength of connections. Since networkx treats higher weights\n",
    "# as 'longer distances', we need to invert our similarity scores.\n",
    "# We create a new attribute 'distance' which is 1/weight.\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if data['weight'] > 0:\n",
    "        data['distance'] = 1.0 / data['weight']\n",
    "    else:\n",
    "        data['distance'] = float('inf')\n",
    "\n",
    "print(\"Calculating Betweenness Centrality (this might take a while)...\")\n",
    "betweenness_centrality = nx.betweenness_centrality(G, weight='distance', normalized=True)\n",
    "\n",
    "# 3. Eigenvector Centrality: Measures influence.\n",
    "print(\"Calculating Eigenvector Centrality...\")\n",
    "# This can sometimes fail to converge on complex graphs. We'll add error handling.\n",
    "try:\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, weight='weight', max_iter=1000)\n",
    "except nx.PowerIterationFailedConvergence:\n",
    "    print(\"Eigenvector centrality did not converge. Filling with 0.\")\n",
    "    eigenvector_centrality = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "print(\"Centrality calculations complete.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- Assemble and Save the Results ---\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "centrality_df = pd.DataFrame({\n",
    "    \"Skill\": list(G.nodes),\n",
    "    \"Degree Centrality\": [degree_centrality.get(node, 0) for node in G.nodes()],\n",
    "    \"Betweenness Centrality\": [betweenness_centrality.get(node, 0) for node in G.nodes()],\n",
    "    \"Eigenvector Centrality\": [eigenvector_centrality.get(node, 0) for node in G.nodes()]\n",
    "})\n",
    "\n",
    "# Sort by a primary centrality measure for easier viewing\n",
    "centrality_df = centrality_df.sort_values(by=\"Degree Centrality\", ascending=False)\n",
    "\n",
    "# Save the final DataFrame to a new CSV file\n",
    "centrality_df.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Successfully created '{OUTPUT_FILE}'.\")\n",
    "print(\"\\nHere's a preview of the top 10 most central skills:\")\n",
    "print(centrality_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a04386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading skills from 'csv/skills_no_duplicate_sorted.csv'...\n",
      "Processing chunk 1...\n",
      "Processing chunk 2...\n",
      "Processing chunk 3...\n",
      "Processing chunk 4...\n",
      "Processing chunk 5...\n",
      "Processing chunk 6...\n",
      "Processing chunk 7...\n",
      "Processing chunk 8...\n",
      "Processing chunk 9...\n",
      "Processing chunk 10...\n",
      "Processing chunk 11...\n",
      "Processing chunk 12...\n",
      "Processing chunk 13...\n",
      "Processing chunk 14...\n",
      "Processing chunk 15...\n",
      "Processing chunk 16...\n",
      "Processing chunk 17...\n",
      "Processing chunk 18...\n",
      "Processing chunk 19...\n",
      "Processing chunk 20...\n",
      "Processing chunk 21...\n",
      "Processing chunk 22...\n",
      "Processing chunk 23...\n",
      "Processing chunk 24...\n",
      "Processing chunk 25...\n",
      "Processing chunk 26...\n",
      "Processing chunk 27...\n",
      "Processing chunk 28...\n",
      "Processing chunk 29...\n",
      "Processing chunk 30...\n",
      "Processing chunk 31...\n",
      "Processing chunk 32...\n",
      "Processing chunk 33...\n",
      "Processing chunk 34...\n",
      "Processing chunk 35...\n",
      "Processing chunk 36...\n",
      "Processing chunk 37...\n",
      "Processing chunk 38...\n",
      "Processing chunk 39...\n",
      "Processing chunk 40...\n",
      "Processing chunk 41...\n",
      "Processing chunk 42...\n",
      "Processing chunk 43...\n",
      "Processing chunk 44...\n",
      "Processing chunk 45...\n",
      "Processing chunk 46...\n",
      "Processing chunk 47...\n",
      "Processing chunk 48...\n",
      "Processing chunk 49...\n",
      "Processing chunk 50...\n",
      "Processing chunk 51...\n",
      "Processing chunk 52...\n",
      "Processing chunk 53...\n",
      "Processing chunk 54...\n",
      "Processing chunk 55...\n",
      "Processing chunk 56...\n",
      "Processing chunk 57...\n",
      "Processing chunk 58...\n",
      "Processing chunk 59...\n",
      "Processing chunk 60...\n",
      "Processing chunk 61...\n",
      "Processing chunk 62...\n",
      "Processing chunk 63...\n",
      "Processing chunk 64...\n",
      "Processing chunk 65...\n",
      "Processing chunk 66...\n",
      "Processing chunk 67...\n",
      "Processing chunk 68...\n",
      "Processing chunk 69...\n",
      "Processing chunk 70...\n",
      "Processing chunk 71...\n",
      "Processing chunk 72...\n",
      "Finished processing skill pairs.\n",
      "Found 3019 unique skills. Building matrix...\n",
      "Matrix built. Normalizing values...\n",
      "Normalization complete. Saving to CSV...\n",
      "Successfully created 'correspendentFinalClean2.csv'!\n"
     ]
    }
   ],
   "source": [
    "# create_matrix.py\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import csv # Import the csv module\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_SKILLS_FILE = \"csv/skills_no_duplicate_sorted.csv\" # Using the cleaned, sorted skills file\n",
    "OUTPUT_MATRIX_FILE = \"correspendentFinalClean2.csv\"\n",
    "\n",
    "print(f\"Reading skills from '{INPUT_SKILLS_FILE}'...\")\n",
    "\n",
    "# Initialize a dictionary to store words and connections as sets to prevent duplicates\n",
    "words = defaultdict(set)\n",
    "\n",
    "# Define the chunk size for reading the CSV in parts\n",
    "chunk_size = 10000\n",
    "i = 0\n",
    "\n",
    "# Process CSV in chunks\n",
    "for chunk in pd.read_csv(INPUT_SKILLS_FILE, header=None, chunksize=chunk_size, low_memory=False):\n",
    "    i += 1\n",
    "    print(f\"Processing chunk {i}...\")\n",
    "    # Iterate over each row in the chunk\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        # Filter out NaN values and convert to list of strings\n",
    "        parts = [str(item) for item in row if pd.notna(item)]\n",
    "        # Generate pairwise combinations and update connections\n",
    "        for a, b in itertools.combinations(parts, 2):\n",
    "            if a and b:\n",
    "                words[a].add(b)\n",
    "                words[b].add(a)\n",
    "\n",
    "print(\"Finished processing skill pairs.\")\n",
    "\n",
    "# Convert keys to a list and determine the size of the final dataset\n",
    "keys = list(words.keys())\n",
    "size = len(keys)\n",
    "key_to_index = {key: i for i, key in enumerate(keys)} # Create a mapping for faster lookups\n",
    "\n",
    "print(f\"Found {size} unique skills. Building matrix...\")\n",
    "\n",
    "# Initialize a numpy array to track connections\n",
    "track = np.zeros((size, size))\n",
    "\n",
    "# Populate the track array based on the accumulated connections\n",
    "for i, k in enumerate(keys):\n",
    "    track[i, i] = len(words[k])  # Self-connection represents the degree of the node\n",
    "    for j in words[k]:\n",
    "        if j in key_to_index: # Ensure the skill is in our keys list\n",
    "            j_index = key_to_index[j]\n",
    "            track[i, j_index] += 1\n",
    "            # The matrix is symmetric, so we don't need to add to track[j_index, i] again\n",
    "\n",
    "print(\"Matrix built. Normalizing values...\")\n",
    "\n",
    "# Normalize each row in track by dividing each element by its diagonal entry\n",
    "# We need to handle division by zero for skills that have no connections (track[row,row] == 0)\n",
    "diagonal = track.diagonal().copy() # Make a copy to avoid modifying it while iterating\n",
    "diagonal[diagonal == 0] = 1 # Avoid division by zero, the result will be 0 anyway\n",
    "\n",
    "for row in range(track.shape[0]):\n",
    "    track[row,:] /= diagonal[row]\n",
    "\n",
    "print(\"Normalization complete. Saving to CSV...\")\n",
    "\n",
    "# Create a DataFrame from the track array with labels for rows and columns\n",
    "track_df = pd.DataFrame(track, index=keys, columns=keys)\n",
    "\n",
    "# Save the DataFrame to a CSV file. Pandas handles the quoting correctly.\n",
    "track_df.to_csv(OUTPUT_MATRIX_FILE)\n",
    "\n",
    "print(f\"Successfully created '{OUTPUT_MATRIX_FILE}'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c4832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Reading skills from 'csv/skills_no_duplicate_sorted.csv' and building co-occurrence map.\n",
      "Step 2: Found 2895 unique skills. Creating the co-occurrence matrix.\n",
      "Step 3: Normalizing the matrix to create correlation scores.\n",
      "Step 4: Saving the final matrix to CSV.\n",
      "\n",
      "Success! Your skill intelligence matrix has been saved to 'skill_co_occurrence_matrix.csv'.\n",
      "This file is the 'brain' of your application.\n",
      "\n",
      "You can now use this file as input for the 'generate_centrality.py' script and then the final 'cv_analyzer_app.py'.\n"
     ]
    }
   ],
   "source": [
    "# generate_matrix.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Configuration ---\n",
    "# This is the file where each row represents a set of co-occurring skills for a project/job.\n",
    "INPUT_SKILLS_FILE = \"csv/skillsFreelancerFinal.csv\" \n",
    "\n",
    "# This will be the final, correctly formatted co-occurrence matrix.\n",
    "OUTPUT_MATRIX_FILE = \"skill_co_occurrence_matrix_with_duplicate.csv\"\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "print(f\"Step 1: Reading skills from '{INPUT_SKILLS_FILE}' and building co-occurrence map.\")\n",
    "\n",
    "# Use defaultdict(set) for efficient and duplicate-free storage of skill connections.\n",
    "skill_connections = defaultdict(set)\n",
    "all_skills = set()\n",
    "\n",
    "# We read the CSV line by line to handle varying numbers of skills per row.\n",
    "with open(INPUT_SKILLS_FILE, 'r', encoding='utf-8') as f:\n",
    "    # Skip the header row if it exists.\n",
    "    # If you are SURE there's no header, you can comment out the next line.\n",
    "    next(f, None) \n",
    "    \n",
    "    for line in f:\n",
    "        # Split by comma and strip whitespace/quotes from each skill\n",
    "        skills_in_row = [skill.strip().strip('\"') for skill in line.strip().split(',')]\n",
    "        \n",
    "        # Filter out any empty strings that might result from trailing commas\n",
    "        cleaned_skills = [skill for skill in skills_in_row if skill]\n",
    "        \n",
    "        # Add all unique skills to our master set\n",
    "        all_skills.update(cleaned_skills)\n",
    "        \n",
    "        # Generate all unique pairs of skills in this row\n",
    "        for skill1, skill2 in itertools.combinations(cleaned_skills, 2):\n",
    "            skill_connections[skill1].add(skill2)\n",
    "            skill_connections[skill2].add(skill1)\n",
    "\n",
    "print(f\"Step 2: Found {len(all_skills)} unique skills. Creating the co-occurrence matrix.\")\n",
    "\n",
    "# Create a sorted list of keys for consistent matrix ordering\n",
    "keys = sorted(list(all_skills))\n",
    "size = len(keys)\n",
    "\n",
    "# Create a mapping of skill_name -> index for much faster lookups\n",
    "key_to_index = {key: i for i, key in enumerate(keys)}\n",
    "\n",
    "# Initialize a numpy array to store the counts\n",
    "co_occurrence_counts = np.zeros((size, size), dtype=int)\n",
    "\n",
    "# Populate the matrix with co-occurrence counts\n",
    "for skill, connections in skill_connections.items():\n",
    "    i = key_to_index[skill]\n",
    "    # The diagonal will store the total number of connections (degree) for each skill\n",
    "    co_occurrence_counts[i, i] = len(connections)\n",
    "    for connected_skill in connections:\n",
    "        if connected_skill in key_to_index:\n",
    "            j = key_to_index[connected_skill]\n",
    "            # We simply count 1 for each co-occurrence\n",
    "            co_occurrence_counts[i, j] = 1\n",
    "\n",
    "print(\"Step 3: Normalizing the matrix to create correlation scores.\")\n",
    "\n",
    "# Create a copy of the matrix for normalization\n",
    "# We will divide each cell (i, j) by the total occurrences of skill i.\n",
    "# This gives P(j|i) - the probability of seeing skill j given that you see skill i.\n",
    "normalized_matrix = np.zeros((size, size), dtype=float)\n",
    "for i in range(size):\n",
    "    total_occurrences = co_occurrence_counts[i, i]\n",
    "    if total_occurrences > 0:\n",
    "        normalized_matrix[i, :] = co_occurrence_counts[i, :] / total_occurrences\n",
    "        normalized_matrix[i, i] = 1.0 # The probability of a skill co-occurring with itself is 1\n",
    "\n",
    "print(\"Step 4: Saving the final matrix to CSV.\")\n",
    "\n",
    "# Create a pandas DataFrame to save the result with proper headers and index\n",
    "final_df = pd.DataFrame(normalized_matrix, index=keys, columns=keys)\n",
    "\n",
    "# Save to CSV. Pandas will automatically handle quoting for skill names with commas.\n",
    "final_df.to_csv(OUTPUT_MATRIX_FILE)\n",
    "\n",
    "print(f\"\\nSuccess! Your skill intelligence matrix has been saved to '{OUTPUT_MATRIX_FILE}'.\")\n",
    "print(\"This file is the 'brain' of your application.\")\n",
    "print(\"\\nYou can now use this file as input for the 'generate_centrality.py' script and then the final 'cv_analyzer_app.py'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dbfcb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading the skill co-occurrence matrix from 'skill_co_occurrence_matrix.csv'...\n",
      "Step 2: Building the skill network graph from the matrix...\n",
      "Initial graph created with 2895 skills and 526253 connections.\n",
      "\n",
      "Step 3: Pruning the graph to keep only the strongest connections...\n",
      "Calculated weight threshold: 0.0090. Edges with weight below this will be removed.\n",
      "Pruned graph has 2895 skills and 26433 strong connections.\n",
      "\n",
      "Step 4: Detecting skill communities using the Louvain algorithm...\n",
      "Successfully identified 387 distinct skill clusters.\n",
      "\n",
      "Step 5: Formatting and saving the clusters to 'skill_clusters.csv'...\n",
      "\n",
      "Success! New 'skill_clusters.csv' has been created.\n",
      "\n",
      "--- Preview of the 5 Largest Clusters Found ---\n",
      "\n",
      "Cluster 0 (Size: 77 skills):\n",
      "ChatGPT, Conversational AI, YOLO, GPT-4, Computer Vision, GPT-3, LLaMA 2, Internet Security, Dolly, Generative AI\n",
      "\n",
      "Cluster 1 (Size: 71 skills):\n",
      "FL Studio, Maya, Logo Design, 3D Animation, Moho, Arts & Crafts, Icon Design, Rotoscoping, Digital Art, Comics\n",
      "\n",
      "Cluster 2 (Size: 70 skills):\n",
      "3D Printing, CNC Machine Retrofitting, Ableton Push, SketchUp, Rhino 3D, BricsCAD, GstarCAD, Ab Initio, CAD/CAM, AliExpress\n",
      "\n",
      "Cluster 3 (Size: 64 skills):\n",
      "Interior Design, Pavement, Frames & Trusses, Kitchen, Resin, Pest Control, Tiling, Landscaping, Power Redesign, Art Consulting\n",
      "\n",
      "Cluster 4 (Size: 58 skills):\n",
      "Google Adwords, Search Engine Marketing, Facebook Verification, Marketo, Link Building, SendGrid, Pipedrive, Google Ads, Google Sheets, Airtable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "import collections\n",
    "\n",
    "# --- Configuration ---\n",
    "# This is your main co-occurrence matrix.\n",
    "# It should be the non-transposed version for easier handling here.\n",
    "# If you only have the transposed one, that's okay, this script will handle it.\n",
    "INPUT_MATRIX_FILE = 'skill_co_occurrence_matrix.csv' # Using the file from your screenshot\n",
    "OUTPUT_CLUSTERS_FILE = 'skill_clusters.csv'\n",
    "\n",
    "# This is the most important parameter to tune.\n",
    "# It defines the percentage of the WEAKEST skill connections to REMOVE before clustering.\n",
    "# A higher value (e.g., 0.90) will create more distinct, smaller clusters.\n",
    "# A lower value (e.g., 0.70) will create fewer, larger clusters.\n",
    "# Let's start high to ensure we break up that single giant cluster.\n",
    "PRUNING_QUANTILE = 0.95 # This means we will only keep the top 5% of strongest connections.\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "print(f\"Step 1: Loading the skill co-occurrence matrix from '{INPUT_MATRIX_FILE}'...\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_MATRIX_FILE, index_col=0)\n",
    "except FileNotFoundError:\n",
    "    print(f\"FATAL ERROR: The input file '{INPUT_MATRIX_FILE}' was not found.\")\n",
    "    print(\"Please make sure this script is in the same directory as your CSV file.\")\n",
    "    exit()\n",
    "\n",
    "# Clean up names to be safe\n",
    "df.columns = df.columns.map(str)\n",
    "df.index = df.index.map(str)\n",
    "\n",
    "print(\"Step 2: Building the skill network graph from the matrix...\")\n",
    "# Create a graph from the pandas DataFrame\n",
    "G = nx.from_pandas_adjacency(df)\n",
    "print(f\"Initial graph created with {G.number_of_nodes()} skills and {G.number_of_edges()} connections.\")\n",
    "\n",
    "print(\"\\nStep 3: Pruning the graph to keep only the strongest connections...\")\n",
    "# Get all edge weights and determine the threshold for keeping the top connections\n",
    "all_weights = [data['weight'] for u, v, data in G.edges(data=True)]\n",
    "if not all_weights:\n",
    "    print(\"Error: No weights found in the graph. Cannot prune.\")\n",
    "    exit()\n",
    "\n",
    "threshold = pd.Series(all_weights).quantile(PRUNING_QUANTILE)\n",
    "print(f\"Calculated weight threshold: {threshold:.4f}. Edges with weight below this will be removed.\")\n",
    "\n",
    "# Create a new, pruned graph\n",
    "G_pruned = nx.Graph()\n",
    "# Add all skills first, so we don't lose any skills that might become isolated\n",
    "G_pruned.add_nodes_from(G) \n",
    "\n",
    "# Add only the edges that are above our threshold\n",
    "for u, v, data in G.edges(data=True):\n",
    "    if data['weight'] >= threshold:\n",
    "        G_pruned.add_edge(u, v, weight=data['weight'])\n",
    "\n",
    "print(f\"Pruned graph has {G_pruned.number_of_nodes()} skills and {G_pruned.number_of_edges()} strong connections.\")\n",
    "\n",
    "print(\"\\nStep 4: Detecting skill communities using the Louvain algorithm...\")\n",
    "# Find communities (clusters) in the pruned graph\n",
    "# The 'seed' makes the result reproducible\n",
    "communities_sets = community.louvain_communities(G_pruned, weight='weight', seed=42)\n",
    "communities_sets = sorted(communities_sets, key=len, reverse=True) # Sort by size\n",
    "\n",
    "print(f\"Successfully identified {len(communities_sets)} distinct skill clusters.\")\n",
    "\n",
    "print(\"\\nStep 5: Formatting and saving the clusters to 'skill_clusters.csv'...\")\n",
    "# Create a dictionary to map each skill to its cluster ID\n",
    "community_dict = {}\n",
    "for i, cluster in enumerate(communities_sets):\n",
    "    for skill in cluster:\n",
    "        community_dict[skill] = i\n",
    "\n",
    "# Convert to a DataFrame\n",
    "community_df = pd.DataFrame(community_dict.items(), columns=[\"Skill\", \"Cluster_ID\"])\n",
    "community_df = community_df.sort_values(by=\"Cluster_ID\")\n",
    "\n",
    "# Save the clusters to a new CSV file\n",
    "community_df.to_csv(OUTPUT_CLUSTERS_FILE, index=False)\n",
    "\n",
    "print(f\"\\nSuccess! New '{OUTPUT_CLUSTERS_FILE}' has been created.\")\n",
    "print(\"\\n--- Preview of the 5 Largest Clusters Found ---\")\n",
    "\n",
    "for i in range(min(5, len(communities_sets))):\n",
    "    cluster_skills = list(communities_sets[i])\n",
    "    print(f\"\\nCluster {i} (Size: {len(cluster_skills)} skills):\")\n",
    "    # Print up to the first 10 skills in the cluster for preview\n",
    "    print(\", \".join(cluster_skills[:10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
